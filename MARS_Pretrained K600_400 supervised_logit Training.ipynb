{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21972,
     "status": "ok",
     "timestamp": 1647615012458,
     "user": {
      "displayName": "Bosch High Prep",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03947787422445982305"
     },
     "user_tz": -330
    },
    "id": "lbKBj806qBZS",
    "outputId": "995737a6-3e7b-4ca0-9884-11a433af244a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/thefurorjuror/MARS.git\n",
    "!cp /content/drive/MyDrive/MARS_UCF101_16f.pth /content/MARS_UCF101_16f.pth\n",
    "!cp -a /content/MARS/. /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqq wandb\n",
    "import wandb\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from dataset.dataset import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import getpass\n",
    "import socket\n",
    "import numpy as np\n",
    "from dataset.preprocess_data import *\n",
    "from PIL import Image, ImageFilter\n",
    "import argparse\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from models.model import generate_model\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import sys\n",
    "import pdb\n",
    "import argparse\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from __future__ import division\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9558,
     "status": "ok",
     "timestamp": 1647411745530,
     "user": {
      "displayName": "Bosch High Prep",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03947787422445982305"
     },
     "user_tz": -330
    },
    "id": "5ANb4cjX2NZD"
   },
   "outputs": [],
   "source": [
    "def init_wandb(model, args=None, pytorch=True) -> None:\n",
    "    \"\"\"\n",
    "    Initialize project on Weights & Biases\n",
    "\n",
    "    Args:\n",
    "        model (Model): Model for Training\n",
    "        args (dict,optional): dict with wandb config. Defaults to None.\n",
    "        wandb_api_key : add your api key\n",
    "        wandb_name : add a unique descriptive name for the run\n",
    "        project : name of wandb project\n",
    "        Sample args : args = {'wandb_api_key': '','wandb_name' : 'test', 'project' : 'test_project'}\n",
    "        pytorch : whether model is in pytorch or tensorflow\n",
    "    \"\"\"\n",
    "    wandb.login(key=args['wandb_api_key'])\n",
    "    wandb.init(\n",
    "        name=args['wandb_name'],\n",
    "        project=args['project'],\n",
    "        resume=True,\n",
    "        dir=\"./\"\n",
    "    )\n",
    "    if pytorch:\n",
    "      \twandb.watch(model, log=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 509,
     "status": "ok",
     "timestamp": 1647615026743,
     "user": {
      "displayName": "Bosch High Prep",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03947787422445982305"
     },
     "user_tz": -330
    },
    "id": "7BfvlZ_uRWRL"
   },
   "outputs": [],
   "source": [
    "def parse_opts():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Datasets \n",
    "    parser.add_argument(\n",
    "        '--frame_dir',\n",
    "        default='dataset/HMDB51/',\n",
    "        type=str,\n",
    "        help='path of jpg files')\n",
    "    parser.add_argument(\n",
    "        '--annotation_path',\n",
    "        default='dataset/HMDB51_labels',\n",
    "        type=str,\n",
    "        help='label paths')\n",
    "    parser.add_argument(\n",
    "        '--dataset',\n",
    "        default='HMDB51',\n",
    "        type=str,\n",
    "        help='(HMDB51, UCF101, Kinectics)')\n",
    "    parser.add_argument(\n",
    "        '--split',\n",
    "        default=1,\n",
    "        type=str,\n",
    "        help='(for HMDB51 and UCF101)')\n",
    "    parser.add_argument(\n",
    "        '--modality',\n",
    "        default='RGB',\n",
    "        type=str,\n",
    "        help='(RGB, Flow)')\n",
    "    parser.add_argument(\n",
    "        '--input_channels',\n",
    "        default=3,\n",
    "        type=int,\n",
    "        help='(3, 2)')\n",
    "    parser.add_argument(\n",
    "        '--n_classes',\n",
    "        default=101,\n",
    "        type=int,\n",
    "        help='Number of classes (activitynet: 200, kinetics: 400, ucf101: 101, hmdb51: 51)')\n",
    "    parser.add_argument(\n",
    "        '--n_finetune_classes',\n",
    "        default=600,\n",
    "        type=int,\n",
    "        help=\n",
    "        'Number of classes for fine-tuning. n_classes is set to the number when pretraining.')\n",
    "    parser.add_argument(\n",
    "        '--only_RGB', \n",
    "        action='store_true', \n",
    "        help='Extracted only RGB frames')\n",
    "    parser.set_defaults(only_RGB = False)\n",
    "    \n",
    "    \n",
    "    # Model parameters\n",
    "    parser.add_argument(\n",
    "        '--output_layers',\n",
    "        action='append',\n",
    "        help='layer to output on forward pass')\n",
    "    parser.set_defaults(output_layers=[])\n",
    "    parser.add_argument(\n",
    "        '--model',\n",
    "        default='resnext',\n",
    "        type=str,\n",
    "        help='Model base architecture')\n",
    "    parser.add_argument(\n",
    "        '--model_depth',\n",
    "        default=101,\n",
    "        type=int,\n",
    "        help='Number of layers in model')\n",
    "    parser.add_argument(\n",
    "        '--resnet_shortcut',\n",
    "        default='B',\n",
    "        type=str,\n",
    "        help='Shortcut type of resnet (A | B)')\n",
    "    parser.add_argument(\n",
    "        '--resnext_cardinality',\n",
    "        default=32,\n",
    "        type=int,\n",
    "        help='ResNeXt cardinality')\n",
    "    parser.add_argument(\n",
    "        '--ft_begin_index',\n",
    "        default=4,\n",
    "        type=int,\n",
    "        help='Begin block index of fine-tuning')\n",
    "    parser.add_argument(\n",
    "        '--sample_size',\n",
    "        default=224,\n",
    "        type=int,\n",
    "        help='Height and width of inputs')\n",
    "    parser.add_argument(\n",
    "        '--sample_duration',\n",
    "        default=16,\n",
    "        type=int,\n",
    "        help='Temporal duration of inputs')\n",
    "    parser.add_argument(\n",
    "        '--training', \n",
    "        action='store_true', \n",
    "        help='training/testing')\n",
    "    parser.set_defaults(training=True)\n",
    "    parser.add_argument(\n",
    "        '--freeze_BN', \n",
    "        action='store_true', \n",
    "        help='freeze_BN/testing')\n",
    "    parser.set_defaults(freeze_BN=False)\n",
    "    parser.add_argument(\n",
    "        '--batch_size', \n",
    "        default=1, \n",
    "        type=int, \n",
    "        help='Batch Size')\n",
    "    parser.add_argument(\n",
    "        '--n_workers', \n",
    "        default=4, \n",
    "        type=int, \n",
    "        help='Number of workers for dataloader')\n",
    "\n",
    "    # optimizer parameters\n",
    "    parser.add_argument(\n",
    "        '--learning_rate',\n",
    "        default=0.1,\n",
    "        type=float,\n",
    "        help='Initial learning rate (divided by 10 while training by lr scheduler)')\n",
    "    parser.add_argument(\n",
    "        '--momentum', \n",
    "        default=0.9, \n",
    "        type=float, \n",
    "        help='Momentum')\n",
    "    parser.add_argument(\n",
    "        '--dampening', \n",
    "        default=0.9, \n",
    "        type=float, \n",
    "        help='dampening of SGD')\n",
    "    parser.add_argument(\n",
    "        '--weight_decay', \n",
    "        default=1e-3, \n",
    "        type=float, \n",
    "        help='Weight Decay')\n",
    "    parser.add_argument(\n",
    "        '--nesterov', \n",
    "        action='store_true', \n",
    "        help='Nesterov momentum')\n",
    "    parser.set_defaults(nesterov=False)\n",
    "    parser.add_argument(\n",
    "        '--optimizer',\n",
    "        default='sgd',\n",
    "        type=str,\n",
    "        help='Currently only support SGD')\n",
    "    parser.add_argument(\n",
    "        '--lr_patience',\n",
    "        default=10,\n",
    "        type=int,\n",
    "        help='Patience of LR scheduler. See documentation of ReduceLROnPlateau.')\n",
    "    parser.add_argument(\n",
    "        '--MARS_alpha', \n",
    "        default=50, \n",
    "        type=float, \n",
    "        help='Weight of Flow augemented MSE loss')\n",
    "    parser.add_argument(\n",
    "        '--n_epochs',\n",
    "        default=400,\n",
    "        type=int,\n",
    "        help='Number of total epochs to run')\n",
    "    parser.add_argument(\n",
    "        '--begin_epoch',\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help='Training begins at this epoch. Previous trained model indicated by resume_path is loaded.')\n",
    "\n",
    "    # options for logging\n",
    "    parser.add_argument(\n",
    "        '--result_path',\n",
    "        default='',\n",
    "        type=str,\n",
    "        help='result_path')\n",
    "    parser.add_argument(\n",
    "        '--MARS', \n",
    "        action='store_true', \n",
    "        help='test MARS')\n",
    "    parser.set_defaults(MARS=False)    \n",
    "    parser.add_argument(\n",
    "        '--pretrain_path', \n",
    "        default='/content/MARS_UCF101_16f.pth', \n",
    "        type=str, \n",
    "        help='Pretrained model (.pth)')\n",
    "    parser.add_argument(\n",
    "        '--MARS_pretrain_path', \n",
    "        default='', \n",
    "        type=str, \n",
    "        help='Pretrained model (.pth)')\n",
    "    parser.add_argument(\n",
    "        '--MARS_resume_path', \n",
    "        default='', \n",
    "        type=str, \n",
    "        help='MARS resume model (.pth)')\n",
    "    parser.add_argument(\n",
    "        '--resume_path1',\n",
    "        default='',\n",
    "        type=str,\n",
    "        help='Save data (.pth) of previous training')\n",
    "    parser.add_argument(\n",
    "        '--resume_path2',\n",
    "        default='',\n",
    "        type=str,\n",
    "        help='Save data (.pth) of previous training')\n",
    "    parser.add_argument(\n",
    "        '--resume_path3',\n",
    "        default='',\n",
    "        type=str,\n",
    "        help='Save data (.pth) of previous training')\n",
    "    parser.add_argument(\n",
    "        '--log',\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help='Log training and validation')\n",
    "    parser.add_argument(\n",
    "        '--checkpoint',\n",
    "        default=2,\n",
    "        type=int,\n",
    "        help='Trained model is saved at every this epochs.')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--manual_seed', default=1, type=int, help='Manually set random seed')\n",
    "    parser.add_argument(\n",
    "        '--random_seed', default=1, type=bool, help='Manually set random seed of sampling validation clip')\n",
    "    \n",
    "    args = parser.parse_args(args = [])\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "qGgJ33-Inp_t"
   },
   "outputs": [],
   "source": [
    "class VideoLogitDataset(Dataset):\n",
    "\n",
    "    def __init__(self, video_dir_path, logits_file, transform=None):\n",
    "\n",
    "        self.video_dir_path = video_dir_path\n",
    "        self.instances = []  # Tensor of image frames\n",
    "        self.logits = pickle.load(open(logits_file, 'rb'))\n",
    "        self.logits = (self.logits).numpy()\n",
    "        self.logits = torch.tensor(self.logits)\n",
    "\n",
    "        self.videos = sorted([str(x.name) for x in Path(self.video_dir_path).iterdir() if x.is_dir()])\n",
    "        self.get_frames()\n",
    "\n",
    "        self.instances = torch.stack(self.instances)\n",
    "        self.num_instances = len(self.instances)\n",
    "        self.transform = transform\n",
    "\n",
    "    def get_frames(self):\n",
    "        for video in tqdm(self.videos, position=0, leave=True):\n",
    "            \n",
    "            image_frames = []\n",
    "            video_dir = os.path.join(self.video_dir_path, video)\n",
    "            if video_dir == '/content/k400_16_frames_uniform/classes.csv' or video_dir == '/content/k400_16_frames_uniform/labels.csv':\n",
    "              continue\n",
    "            images = os.listdir(video_dir)\n",
    "\n",
    "            for image_name in images:\n",
    "                image = Image.open(os.path.join(video_dir, image_name))\n",
    "                # image = np.array(image, dtype=np.float32)\n",
    "                newsize = (224,224)\n",
    "                image = image.resize(newsize)\n",
    "                image = np.array(image, dtype=np.float32)\n",
    "                image = image / 255.0\n",
    "                image_frames.append(torch.tensor(image))\n",
    "\n",
    "            self.instances.append(torch.stack(image_frames))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vid = self.instances[idx]\n",
    "        vid = vid.swapaxes(0, 3)\n",
    "        if self.transform:\n",
    "            vid = self.transform(vid)\n",
    "        return vid, self.logits[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "\n",
    "class ValDataset(Dataset):\n",
    "\n",
    "    def __init__(self, video_dir_path, classes_file, labels_file, num_classes, transform=None):\n",
    "\n",
    "        self.video_dir_path = video_dir_path\n",
    "        self.classes_file = classes_file\n",
    "        self.labels_file = labels_file\n",
    "        self.transform = transform\n",
    "\n",
    "        self.videos = sorted([str(x.name) for x in Path(self.video_dir_path).iterdir() if x.is_dir()])\n",
    "        self.num_instances = len(self.videos)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.label_dict = pd.read_csv(self.labels_file, header=None, index_col=1, squeeze=False).to_dict()\n",
    "        self.label_dict = self.label_dict[0]\n",
    "\n",
    "        self.classes_dict = pd.read_csv(self.classes_file, header=None, index_col=1, squeeze=False).to_dict()\n",
    "        self.classes_dict = self.classes_dict[0]\n",
    "\n",
    "        self.new_classes_dict = {}\n",
    "        for index, (id, label) in enumerate(self.classes_dict.items()):\n",
    "            if index == 0:\n",
    "                continue\n",
    "            self.new_classes_dict[id] = self.label_dict[label]\n",
    "        print(self.new_classes_dict)\n",
    "        print(len(self.new_classes_dict))\n",
    "\n",
    "    def get_id(self, video_name):\n",
    "        k = 0\n",
    "        rev = video_name[::-1]\n",
    "        for x in range(len(video_name)):\n",
    "            if rev[x] == '_':\n",
    "                k = k + 1\n",
    "            if k >= 2:\n",
    "                k = x\n",
    "                break\n",
    "\n",
    "        id = video_name[0:len(video_name) - k - 1]\n",
    "\n",
    "        return id\n",
    "\n",
    "    def get_label(self, idx):\n",
    "        video_name = self.videos[idx]\n",
    "        video_id = self.get_id(video_name)\n",
    "        label = self.new_classes_dict[video_id]\n",
    "        one_hot = F.one_hot(torch.tensor(int(label)), self.num_classes)\n",
    "        return one_hot\n",
    "\n",
    "    def get_frames(self, video_path):\n",
    "        images = sorted(os.listdir(video_path))\n",
    "        image_frames = []\n",
    "\n",
    "        for image_name in images:\n",
    "            image = Image.open(os.path.join(video_path, image_name))\n",
    "            newsize = (224,224)\n",
    "            image = image.resize(newsize)\n",
    "            image = np.array(image, dtype=np.float32)\n",
    "            image = image / 255.0\n",
    "            image_frames.append(torch.tensor(image))\n",
    "\n",
    "        return torch.stack(image_frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = os.path.join(self.video_dir_path, self.videos[idx])\n",
    "        vid = self.get_frames(video_path)\n",
    "        if self.transform:\n",
    "            vid = vid.permute(0, 3, 1, 2)\n",
    "            vid = self.transform(vid)\n",
    "            vid = vid.permute(0, 2, 3, 1)\n",
    "        vid = vid.swapaxes(0, 3)  # <C3D Transform>\n",
    "        return vid, self.get_label(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_instances\n",
    "\n",
    "class ValDataset_var_frames(Dataset):\n",
    "\n",
    "    def __init__(self, video_dir_path, classes_file, labels_file, num_classes,frames_needed, transform=None):\n",
    "\n",
    "        self.video_dir_path = video_dir_path\n",
    "        self.classes_file = classes_file\n",
    "        self.labels_file = labels_file\n",
    "        self.transform = transform\n",
    "        self.frames_needed = frames_needed\n",
    "\n",
    "        self.videos = sorted([str(x.name) for x in Path(self.video_dir_path).iterdir() if x.is_dir()])\n",
    "        self.num_instances = len(self.videos)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.label_dict = pd.read_csv(self.labels_file, header=None, index_col=1, squeeze=False).to_dict()\n",
    "        self.label_dict = self.label_dict[0]\n",
    "\n",
    "        self.classes_dict = pd.read_csv(self.classes_file, header=None, index_col=1, squeeze=False).to_dict()\n",
    "        self.classes_dict = self.classes_dict[0]\n",
    "\n",
    "        self.new_classes_dict = {}\n",
    "        for index, (id, label) in enumerate(self.classes_dict.items()):\n",
    "            if index == 0:\n",
    "                continue\n",
    "            self.new_classes_dict[id] = self.label_dict[label]\n",
    "        print(self.new_classes_dict)\n",
    "        print(len(self.new_classes_dict))\n",
    "\n",
    "    def get_id(self, video_name):\n",
    "        k = 0\n",
    "        rev = video_name[::-1]\n",
    "        for x in range(len(video_name)):\n",
    "            if rev[x] == '_':\n",
    "                k = k + 1\n",
    "            if k >= 2:\n",
    "                k = x\n",
    "                break\n",
    "\n",
    "        id = video_name[0:len(video_name) - k - 1]\n",
    "\n",
    "        return id\n",
    "\n",
    "    def get_label(self, idx):\n",
    "        video_name = self.videos[idx]\n",
    "        video_id = self.get_id(video_name)\n",
    "        label = self.new_classes_dict[video_id]\n",
    "        one_hot = F.one_hot(torch.tensor(int(label)), self.num_classes)\n",
    "        return one_hot\n",
    "\n",
    "    def get_frames(self, video_path):\n",
    "        images = sorted(os.listdir(video_path))\n",
    "        image_frames = []\n",
    "        frames_avlb = len(images)\n",
    "        print('frame_avlb',frames_avlb)\n",
    "        if frames_avlb == 0:\n",
    "          image = np.ones((224,224))\n",
    "          for i in range(self.frames_needed):\n",
    "            image_frames.append(torch.tensor(image))\n",
    "          return torch.stack(image_frames)\n",
    "        reps = int(self.frames_needed/frames_avlb)\n",
    "        rem = self.frames_needed % frames_avlb\n",
    "        rem = reps + rem\n",
    "        \n",
    "        if frames_avlb >=self.frames_needed :\n",
    "          images = images[:self.frames_needed]\n",
    "          for image_name in images:\n",
    "            image = Image.open(os.path.join(video_path, image_name))\n",
    "            newsize = (224,224)\n",
    "            image = image.resize(newsize)\n",
    "            image = np.array(image, dtype=np.float32)\n",
    "            image = image / 255.0\n",
    "            image_frames.append(torch.tensor(image))\n",
    "        else : \n",
    "          print('here')\n",
    "          c=0\n",
    "          for image_name in images:\n",
    "              image = Image.open(os.path.join(video_path, image_name))\n",
    "              newsize = (224,224)\n",
    "              image = image.resize(newsize)\n",
    "              image = np.array(image, dtype=np.float32)\n",
    "              image = image / 255.0\n",
    "              c=c+1\n",
    "              if (c==0):\n",
    "                for i in range(rem):\n",
    "                  image_frames.append(torch.tensor(image))\n",
    "              else:\n",
    "                for i in range(reps):\n",
    "                  image_frames.append(torch.tensor(image))\n",
    "\n",
    "        return torch.stack(image_frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = os.path.join(self.video_dir_path, self.videos[idx])\n",
    "        vid = self.get_frames(video_path)\n",
    "        if self.transform:\n",
    "            vid = vid.permute(0, 3, 1, 2)\n",
    "            vid = self.transform(vid)\n",
    "            vid = vid.permute(0, 2, 3, 1)\n",
    "        vid = vid.swapaxes(0, 3)  # <C3D Transform>\n",
    "        return vid, self.get_label(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_instances\n",
    "\n",
    "def extrapolate(input_dir, output_dir, out_frames: int = 16):\n",
    "    input_dir = Path(input_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    error_count = 0\n",
    "    videos = sorted(os.listdir(input_dir))\n",
    "\n",
    "    for video in tqdm(videos):\n",
    "\n",
    "        frames = sorted(os.listdir(input_dir / video))\n",
    "        if len(frames) == 0:\n",
    "            print(f'----> Skipping {video}: video has no frames')\n",
    "            continue\n",
    "        else:\n",
    "            frames = sorted(frames * (out_frames // len(frames)))\n",
    "            new_vid_frames = frames\n",
    "            length = len(new_vid_frames)\n",
    "            add_frames = out_frames % length\n",
    "            x = length // (add_frames + 1)\n",
    "            a = x\n",
    "\n",
    "        if add_frames % 2 != 0:\n",
    "            new_vid_frames.append(frames[length // 2])\n",
    "            add_frames = add_frames - 1\n",
    "\n",
    "        while add_frames != 0:\n",
    "            new_vid_frames.append(frames[a - 1])\n",
    "            new_vid_frames.append(frames[length - a])\n",
    "            a = a + x\n",
    "            add_frames = add_frames - 2\n",
    "\n",
    "        new_vid_frames.sort()\n",
    "\n",
    "        out_path = output_dir / video\n",
    "        out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        k = 0\n",
    "        for idx, frame in enumerate(new_vid_frames):\n",
    "            src = input_dir / video / frame\n",
    "            dst = output_dir / video / (str(idx) + \".jpg\")\n",
    "            shutil.copy(src, dst)\n",
    "        if len(os.listdir(output_dir / video)) != 16:\n",
    "            print(len(new_vid_frames))\n",
    "            error_count += 1\n",
    "    if error_count > 0:\n",
    "        print(f'----> {error_count} videos were not copied')\n",
    "    else:\n",
    "        print('----> All videos were copied')\n",
    "class VideoLogitDataset_noise(Dataset):\n",
    "\n",
    "    def __init__(self, video_dir_path, csv_file):\n",
    "        self.video_dir_path = video_dir_path\n",
    "        self.csv = pd.read_csv(csv_file)\n",
    "\n",
    "        self.videos = self.csv['FileNames']\n",
    "        self.videos = [self.video_dir_path + x + '.pkl' for x in self.videos]\n",
    "        self.labels = self.csv['Labels']\n",
    "\n",
    "        self.num_instances = len(self.csv)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames = torch.tensor(pickle.load(open(self.videos[idx], 'rb'))).to(torch.float32)\n",
    "        label = np.zeros((600))\n",
    "        label[self.labels[idx]] = 1\n",
    "        label = torch.tensor(label)\n",
    "        return frames, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.num_instances)\n",
    "\n",
    "class VideoLogitDataset_cgan_noise(Dataset):\n",
    "\n",
    "    def __init__(self, video_dir_path, logits_file):\n",
    "        self.video_dir_path = video_dir_path\n",
    "        self.num_instances = len(os.listdir(self.video_dir_path))\n",
    "        self.logits = pickle.load(open(logits_file, 'rb'))\n",
    "        self.logits = (self.logits).numpy()\n",
    "        self.logits = torch.tensor(self.logits)\n",
    "\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        frame = np.array(Image.open(self.video_dir_path+\"/\"+str(idx)+\"/\"+str(idx)+\".png\"))\n",
    "        frame = torch.tensor(frame.reshape((frame.shape[2],1,frame.shape[0],frame.shape[1])))\n",
    "        return frame.to(torch.float32), self.logits[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.num_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "95W8MmWMxrd_"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class Logger(object):\n",
    "\n",
    "    def __init__(self, path, header, resume_path, begin_epoch):\n",
    "        if (not os.path.exists(path)) or (resume_path==''):\n",
    "            self.log_file = open(path, 'w+')\n",
    "            self.logger = csv.writer(self.log_file, delimiter='\\t')\n",
    "            self.logger.writerow(header)\n",
    "        else:\n",
    "            self.log_file = open(path, 'r+')\n",
    "            self.logger = csv.writer(self.log_file, delimiter='\\t')\n",
    "            reader = csv.reader(self.log_file, delimiter='\\t')\n",
    "            lines = []\n",
    "            print(\"begin = \", begin_epoch)\n",
    "            for line in reader:\n",
    "                lines.append(line)\n",
    "                if len(lines) == begin_epoch +1 :\n",
    "                    break\n",
    "            self.log_file.close()\n",
    "            self.log_file = open(path, 'w')\n",
    "            self.logger = csv.writer(self.log_file, delimiter='\\t')\n",
    "            self.logger.writerows(lines[:begin_epoch+1])\n",
    "            self.log_file.flush()\n",
    "            \n",
    "        self.header = header\n",
    "\n",
    "\n",
    "    def __del(self):\n",
    "        self.log_file.close()\n",
    "\n",
    "    def log(self, values):\n",
    "        write_values = []\n",
    "        for col in self.header:\n",
    "            assert col in values\n",
    "            write_values.append(values[col])\n",
    "\n",
    "        self.logger.writerow(write_values)\n",
    "        self.log_file.flush()\n",
    "\n",
    "class Logger_MARS(object):\n",
    "\n",
    "    def __init__(self, path, header, resume_path, begin_epoch):\n",
    "        if resume_path == '':\n",
    "            self.log_file = open(path, 'w+')\n",
    "            self.logger = csv.writer(self.log_file, delimiter='\\t')\n",
    "            self.logger.writerow(header)\n",
    "        else:\n",
    "            self.log_file = open(path, 'r+')\n",
    "            self.logger = csv.writer(self.log_file, delimiter='\\t')\n",
    "            reader = csv.reader(self.log_file, delimiter='\\t')\n",
    "            lines = []\n",
    "            print(\"begin = \", begin_epoch)\n",
    "            for line in reader:\n",
    "                lines.append(line)\n",
    "                if len(lines) == begin_epoch +1 :\n",
    "                    break\n",
    "            self.log_file.close()\n",
    "            self.log_file = open(path, 'w')\n",
    "            self.logger = csv.writer(self.log_file, delimiter='\\t')\n",
    "            self.logger.writerows(lines[:begin_epoch+1])\n",
    "            self.log_file.flush()\n",
    "\n",
    "        self.header = header\n",
    "\n",
    "\n",
    "    def __del(self):\n",
    "        self.log_file.close()\n",
    "\n",
    "    def log(self, values):\n",
    "        write_values = []\n",
    "        for col in self.header:\n",
    "            assert col in values\n",
    "            write_values.append(values[col])\n",
    "\n",
    "        self.logger.writerow(write_values)\n",
    "        self.log_file.flush()\n",
    "\n",
    "\n",
    "def load_value_file(file_path):\n",
    "    with open(file_path, 'r') as input_file:\n",
    "        value = float(input_file.read().rstrip('\\n\\r'))\n",
    "\n",
    "    return value\n",
    "\n",
    "def calculate_accuracy(outputs, targets):\n",
    "    batch_size = targets.size(0)\n",
    "    _, pred = outputs.topk(1, 1, True)\n",
    "    pred = pred.t()\n",
    "    _, targets = targets.topk(1, 1, True)\n",
    "    targets = targets.t()\n",
    "    correct = pred.eq(targets.view(1, -1))\n",
    "\n",
    "    n_correct_elems = correct.float().sum().item()\n",
    "    \n",
    "\n",
    "    return n_correct_elems / batch_size\n",
    "\n",
    "def calculate_accuracy5(output, target, topk=5):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    k = topk\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(k, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    _, targets = target.topk(1, 1, True)\n",
    "    targets = targets.t()\n",
    "    correct = pred.eq(targets.view(1, -1).expand_as(pred))\n",
    "    correct_k = correct[:k].view(-1).float().sum(0)\n",
    "    return correct_k.mul_(1.0 / batch_size)\n",
    "    \n",
    "def calculate_accuracy_video(output_buffer, i):\n",
    "    true_value = output_buffer[: i+1,-1]\n",
    "    pred_value = np.argmax(output_buffer[:i+1, :-1], axis = 1)\n",
    "#    print(output_buffer[0:3,:])\n",
    "    # print(true_value)\n",
    "    # print(pred_value)\n",
    "    # print(\"accuracy = \", 1*(np.equal(true_value, pred_value)).sum()/len(true_value))\n",
    "    return 1*(np.equal(true_value, pred_value)).sum()/len(true_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1647614987680,
     "user": {
      "displayName": "Bosch High Prep",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03947787422445982305"
     },
     "user_tz": -330
    },
    "id": "r4hRozFy2OQp"
   },
   "outputs": [],
   "source": [
    "args = {'wandb_api_key': 'f6dd820ca08b228c7004a5478d1d3ccd01fcbea2','wandb_name' : 'grey_box_ucf_pretrained_k600_training', 'project' : 'model_extraction'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4zQxUrZes-O",
    "outputId": "0417beee-e562-4a5b-eefc-61693e4bb0b5"
   },
   "outputs": [],
   "source": [
    "opt = parse_opts()\n",
    "print(opt)\n",
    "\n",
    "opt.arch = '{}-{}'.format(opt.model, opt.model_depth)\n",
    "torch.manual_seed(opt.manual_seed)\n",
    "\n",
    "if opt.modality=='RGB': opt.input_channels = 3\n",
    "elif opt.modality=='Flow': opt.input_channels = 2\n",
    "\n",
    "print(\"Loading model... \", opt.model, opt.model_depth)\n",
    "model, parameters = generate_model(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "XzMNnO8C3gBp"
   },
   "outputs": [],
   "source": [
    "init_wandb(model,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "EASMQ8c0yTPW"
   },
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/kinetics_final/k400/k400_train_5_percent_16_frames_uniform.zip /content/\n",
    "!unzip -q /content/k400_train_5_percent_16_frames_uniform.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j04Na41KjVyg"
   },
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/kinetics_final/k400/k400_val_16_frames_uniform.zip /content/\n",
    "!unzip -q /content/k400_val_16_frames_uniform.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_dataset_aug = VideoLogitDataset('/content/Videos', '/content/drive/MyDrive/annotations/train_k400_5_percent_list.txt', '/content/drive/MyDrive/logits/swin_transformer_k400_train5precent_2.pkl')\n",
    "val_dataset = ValDataset('/content/k400_16_frames_uniform', '/content/k400_16_frames_uniform/classes.csv', '/content/k400_16_frames_uniform/labels.csv', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "FUM1z80JCTkv"
   },
   "outputs": [],
   "source": [
    "train_data = finetune_dataset\n",
    "val_data = val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "CrRXrXRJCdRq"
   },
   "outputs": [],
   "source": [
    "print(\"Preparing datatloaders ...\")\n",
    "train_dataloader = DataLoader(train_data, batch_size = opt.batch_size, shuffle=True, num_workers = opt.n_workers, pin_memory = True)\n",
    "val_dataloader   = DataLoader(val_data, batch_size = opt.batch_size, shuffle=True, num_workers = opt.n_workers, pin_memory = True)\n",
    "print(\"Length of train datatloader = \",len(train_dataloader))\n",
    "print(\"Length of validation datatloader = \",len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt.pretrain_path: \n",
    "    opt.weight_decay = 1e-5\n",
    "    opt.learning_rate = 0.001\n",
    "\n",
    "if opt.nesterov: dampening = 0\n",
    "else: dampening = opt.dampening\n",
    "    \n",
    "print(\"lr = {} \\t momentum = {} \\t dampening = {} \\t weight_decay = {}, \\t nesterov = {}\"\n",
    "            .format(opt.learning_rate, opt.momentum, dampening, opt. weight_decay, opt.nesterov))\n",
    "print(\"LR patience = \", opt.lr_patience)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    parameters,\n",
    "    lr=opt.learning_rate,\n",
    "    momentum=opt.momentum,\n",
    "    dampening=dampening,\n",
    "    weight_decay=opt.weight_decay,\n",
    "    nesterov=opt.nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "7EaxWpg0wFcs"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=opt.lr_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "EvcDm24phboS",
    "outputId": "6087ebec-a6f2-436c-afec-620ae5f7094c"
   },
   "outputs": [],
   "source": [
    "print('run')\n",
    "best_val_acc = 0\n",
    "for epoch in range(1, opt.n_epochs + 1):\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  batch_time = AverageMeter()\n",
    "  data_time = AverageMeter()\n",
    "  losses = AverageMeter()\n",
    "  accuracies = AverageMeter()\n",
    "\n",
    "  end_time = time.time()\n",
    "\n",
    "  for i, (inputs, targets) in enumerate(train_dataloader):\n",
    "      data_time.update(time.time() - end_time)\n",
    "      inputs = torch.permute(inputs,(0,1,4,2,3))\n",
    "      targets = torch.nn.functional.softmax(targets)\n",
    "      targets = targets.to(torch.float32)\n",
    "      targets = targets.cuda(non_blocking=True)\n",
    "      inputs = Variable(inputs)\n",
    "      targets = Variable(targets)\n",
    "      outputs = model(inputs)\n",
    "\n",
    "      loss = criterion(outputs, targets)\n",
    "      acc = calculate_accuracy(outputs, targets)\n",
    "\n",
    "      losses.update(loss.item(), inputs.size(0))\n",
    "      accuracies.update(acc, inputs.size(0))\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      batch_time.update(time.time() - end_time)\n",
    "      end_time = time.time()\n",
    "\n",
    "      print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "            'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "            'Aug Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "            'Aug Acc {acc.val:.3f} ({acc.avg:.3f})'.format(\n",
    "                epoch,\n",
    "                i + 1,\n",
    "                len(train_dataloader),\n",
    "                batch_time=batch_time,\n",
    "                data_time=data_time,\n",
    "                loss=losses,\n",
    "                acc=accuracies))\n",
    "      wandb.log({\n",
    "        'Training loss': losses.avg,\n",
    "        'Training Accuracy': accuracies.avg,}, step=epoch)\n",
    "  \n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  batch_time = AverageMeter()\n",
    "  data_time = AverageMeter()\n",
    "  losses = AverageMeter()\n",
    "  accuracies = AverageMeter()\n",
    "\n",
    "  end_time = time.time()\n",
    "  with torch.no_grad():\n",
    "      for i, (inputs, targets) in enumerate(val_dataloader):\n",
    "          inputs = torch.permute(inputs,(0,1,4,2,3))\n",
    "          # targets = torch.nn.functional.softmax(targets)\n",
    "          targets = targets.to(torch.float32)\n",
    "          # pdb.set_trace()\n",
    "          data_time.update(time.time() - end_time)\n",
    "          targets = targets.cuda(non_blocking=True)\n",
    "          inputs = Variable(inputs)\n",
    "          targets = Variable(targets)\n",
    "          outputs = model(inputs)\n",
    "          loss = criterion(outputs, targets)\n",
    "          acc = calculate_accuracy(outputs, targets)\n",
    "      \n",
    "          losses.update(loss.item(), inputs.size(0))\n",
    "          accuracies.update(acc, inputs.size(0))\n",
    "\n",
    "          batch_time.update(time.time() - end_time)\n",
    "          end_time = time.time()\n",
    "\n",
    "          print('Val_Epoch: [{0}][{1}/{2}]\\t'\n",
    "              'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "              'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "              'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "              'Acc {acc.val:.3f} ({acc.avg:.3f})'.format(\n",
    "                  epoch,\n",
    "                  i + 1,\n",
    "                  len(val_dataloader),\n",
    "                  batch_time=batch_time,\n",
    "                  data_time=data_time,\n",
    "                  loss=losses,\n",
    "                  acc=accuracies))\n",
    "          wandb.log({\n",
    "        'Validation loss': losses.avg,\n",
    "        'Validation Accuracy': accuracies.avg,}, step=epoch)\n",
    "  if(accuracies.avg > best_val_acc):\n",
    "        best_val_acc = accuracies.avg\n",
    "        torch.save(model.module.state_dict(),'/checkpoints/best_model.pth')\n",
    "\n",
    "  if epoch%2==0:\n",
    "    torch.save(model.module.state_dict(),'/checkpoints/'+ str(epoch) +'.pth')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MARS_Pretrained K600/400 supervised/logit Testing",
   "provenance": [
    {
     "file_id": "1MP3fPv3RMm-CoDdQ-8JP_guD44d6lEZb",
     "timestamp": 1647334190695
    },
    {
     "file_id": "1yRqyyE7H-dLLaMr7a_GkdnpYkca5TDVP",
     "timestamp": 1647245403303
    },
    {
     "file_id": "1UMHyXHJpOwDlrpafEWaPoDN6tG0d5KDC",
     "timestamp": 1646972587868
    },
    {
     "file_id": "1PaN6zMvtG1_LA3RgtYx_6DbnLoZX3LIE",
     "timestamp": 1646888503854
    }
   ],
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
