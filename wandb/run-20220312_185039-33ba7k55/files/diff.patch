diff --git a/DFAD/DFAD_video.py b/DFAD/DFAD_video.py
index 144302c..bfcfe4a 100644
--- a/DFAD/DFAD_video.py
+++ b/DFAD/DFAD_video.py
@@ -44,6 +44,7 @@ def train(args, teacher, student, generator, device, optimizer, epoch):
             fake = torch.sigmoid(generator(z).detach())
             fake_shape = fake.shape
 
+            
             t_logit = torch.tensor(teacher(fake)).to(device)
 
             fake = fake.view(fake_shape[0], fake_shape[2], fake_shape[1], fake_shape[3], fake_shape[4])
diff --git a/DFAD/DFAD_video_swapped.py b/DFAD/DFAD_video_swapped.py
index d0ee8f0..f4ac162 100644
--- a/DFAD/DFAD_video_swapped.py
+++ b/DFAD/DFAD_video_swapped.py
@@ -12,6 +12,7 @@ import torch.nn as nn
 from tqdm import tqdm
 import torch.optim as optim
 import torch.nn.functional as F
+import torchvision.transforms.functional as TF
 
 # Import for Swin-T
 from mmcv import Config
@@ -43,7 +44,20 @@ def train(args, teacher, student, generator, device, optimizer, epoch):
           fake = torch.sigmoid(generator(z))
           fake_shape = fake.shape
 
-          t_logit = torch.tensor(teacher(fake)).to(device)
+          fake_teacher = fake.reshape((-1, fake_shape[2], fake_shape[1], *fake_shape[3:]))
+          fake_teacher_batch = []
+          for vid in list(fake_teacher):
+            vid = torch.stack(
+              [
+                TF.normalize(frame*255, [123.675, 116.28, 103.53], [58.395, 57.12, 57.375]) 
+                for frame in vid
+                ]
+              )
+            fake_teacher_batch.append(vid)
+          fake_teacher = torch.stack(fake_teacher_batch)
+          
+          fake_teacher = fake_teacher.reshape((-1, 1, *fake_shape[1:]))
+          t_logit = torch.tensor(teacher(fake_teacher, return_loss=False)).to(device)
 
           fake = fake.view(fake_shape[0], fake_shape[2], fake_shape[1], fake_shape[3], fake_shape[4])
           s_logit = student(fake).to(device)
@@ -64,7 +78,19 @@ def train(args, teacher, student, generator, device, optimizer, epoch):
         fake = torch.sigmoid(generator(z).detach())
         fake_shape = fake.shape
 
-        t_logit = torch.tensor(teacher(fake)).to(device)
+        fake_teacher = fake.reshape((-1, fake_shape[2], fake_shape[1], *fake_shape[3:]))
+        fake_teacher_batch = []
+        for vid in list(fake_teacher):
+          vid = torch.stack(
+            [
+              TF.normalize(frame*255, [123.675, 116.28, 103.53], [58.395, 57.12, 57.375]) 
+              for frame in vid
+              ]
+            )
+          fake_teacher_batch.append(vid)
+        fake_teacher = torch.stack(fake_teacher_batch)
+        fake_teacher = fake_teacher.reshape((-1, 1, *fake_shape[1:]))
+        t_logit = torch.tensor(teacher(fake_teacher, return_loss=False)).to(device)
 
         fake = fake.view(fake_shape[0], fake_shape[2], fake_shape[1], fake_shape[3], fake_shape[4])
         s_logit = student(fake).to(device)
@@ -101,7 +127,7 @@ def main():
 
     # Training settings
     parser = argparse.ArgumentParser(description='DFAD MNIST')
-    parser.add_argument('--batch_size', type=int, default=16, metavar='N',
+    parser.add_argument('--batch_size', type=int, default=4, metavar='N',
                         help='input batch size for training (default: 64)')
     parser.add_argument('--epochs', type=int, default=40, metavar='N',
                         help='number of epochs to train (default: 40)')
@@ -157,7 +183,7 @@ def main():
 
     if args.model_name == "swin-t":
         print()
-        config = "./VST/configs/_base_/models/swin/swin_tiny.py"
+        config = "./Video-Swin-Transformer/configs/recognition/swin/swin_tiny_patch244_window877_kinetics400_1k.py"
         checkpoint = "/content/swin_tiny_patch244_window877_kinetics400_1k.pth"
         cfg = Config.fromfile(config)
         teacher = build_model(cfg.model, train_cfg=None, test_cfg=cfg.get('test_cfg'))
diff --git a/DFAD/network/models.py b/DFAD/network/models.py
index 308b427..b22ed12 100644
--- a/DFAD/network/models.py
+++ b/DFAD/network/models.py
@@ -129,7 +129,7 @@ class VideoGAN(nn.Module):
         # self.conv5b = nn.ConvTranspose2d(64, 3, [4,4], [2,2], [1,1])
 
         # Foreground
-        self.conv1 = nn.ConvTranspose3d(zdim, 512, [1, 4, 4], [1, 1, 1])
+        self.conv1 = nn.ConvTranspose3d(zdim, 512, [1, 7, 7], [1, 1, 1])
         self.bn1 = nn.BatchNorm3d(512)
 
         self.conv2 = nn.ConvTranspose3d(512, 256, [4, 4, 4], [2, 2, 2], [1, 1, 1])
@@ -141,7 +141,10 @@ class VideoGAN(nn.Module):
         self.conv4 = nn.ConvTranspose3d(128, 64, [4, 4, 4], [2, 2, 2], [1, 1, 1])
         self.bn4 = nn.BatchNorm3d(64)
 
-        self.conv5 = nn.ConvTranspose3d(64, 3, [4, 4, 4], [2, 2, 2], [1, 1, 1])
+        self.conv5 = nn.ConvTranspose3d(64, 32, [4, 4, 4], [2, 2, 2], [1, 1, 1])
+        self.bn5 = nn.BatchNorm3d(32)
+
+        self.conv6 = nn.ConvTranspose3d(32, 3, [4, 4, 4], [2, 2, 2], [1, 1, 1])
 
         # Mask
         # self.conv5m = nn.ConvTranspose3d(64, 1, [4,4,4], [2,2,2], [1,1,1])
@@ -169,9 +172,10 @@ class VideoGAN(nn.Module):
         f = F.leaky_relu(self.bn2(self.conv2(f)))
         f = F.leaky_relu(self.bn3(self.conv3(f)))
         f = F.leaky_relu(self.bn4(self.conv4(f)))
+        f = F.leaky_relu(self.bn5(self.conv5(f)))
         # m = torch.sigmoid(self.conv5m(f))   # b, 1, 32, 64, 64
         # Model Update: Removed activation function from last year
-        f = self.conv5(f)  # b, 3, 32, 64, 64
+        f = self.conv6(f)  # b, 3, 32, 64, 64
 
         # out = m*f + (1-m)*b
         out = f
diff --git a/Video-Swin-Transformer/configs/recognition/swin/swin_tiny_patch244_window877_kinetics400_1k.py b/Video-Swin-Transformer/configs/recognition/swin/swin_tiny_patch244_window877_kinetics400_1k.py
index 69c7e0b..b2a77ce 100644
--- a/Video-Swin-Transformer/configs/recognition/swin/swin_tiny_patch244_window877_kinetics400_1k.py
+++ b/Video-Swin-Transformer/configs/recognition/swin/swin_tiny_patch244_window877_kinetics400_1k.py
@@ -1,7 +1,7 @@
 _base_ = [
     '../../_base_/models/swin/swin_tiny.py', '../../_base_/default_runtime.py'
 ]
-model=dict(backbone=dict(patch_size=(2,4,4), drop_path_rate=0.1), test_cfg=dict(max_testing_views=4))
+model=dict(backbone=dict(patch_size=(2,4,4), drop_path_rate=0.1), test_cfg=dict())
 
 # dataset settings
 dataset_type = 'VideoDataset'
diff --git a/wandb/latest-run b/wandb/latest-run
index d1071c5..9090e1e 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20220310_060756-18r5a49k
\ No newline at end of file
+run-20220312_185039-33ba7k55
\ No newline at end of file
